{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Copy of practice_reinforce_pytorch.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_reinforce_pytorch.ipynb","timestamp":1590091280284}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"N72QAMfO00OO","colab_type":"text"},"source":["# REINFORCE in pytorch\n","\n","Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"id":"OosJvRfA00OR","colab_type":"code","outputId":"644d13d6-2bc8-4e01-e9bd-8e53ee4dc4a4","executionInfo":{"status":"ok","timestamp":1590104741108,"user_tz":300,"elapsed":1793,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import sys, os\n","if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n","\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n","\n","    !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Starting virtual X frame buffer: Xvfb.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-Vid5Ljs00Of","colab_type":"code","outputId":"3db7132e-7950-4c53-e03e-a4b011c24512","executionInfo":{"status":"ok","timestamp":1590104742753,"user_tz":300,"elapsed":1179,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"}},"colab":{"base_uri":"https://localhost:8080/","height":287}},"source":["import gym\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","env = gym.make(\"CartPole-v0\").env\n","env.reset()\n","n_actions =  env.action_space.n\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f08d4866f28>"]},"metadata":{"tags":[]},"execution_count":2},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"kxZxrv-100Os","colab_type":"text"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"XupWrlwE00Ou","colab_type":"text"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."]},{"cell_type":"code","metadata":{"id":"wMm4rRI600Ov","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ac0pIHnT00O4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"2277b791-6a9e-4ec6-e701-67c7683f69f1","executionInfo":{"status":"ok","timestamp":1590104748754,"user_tz":300,"elapsed":3514,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"}}},"source":["# Build a simple neural network that predicts policy logits. \n","# Keep it simple: CartPole isn't worth deep architectures.\n","model = nn.Sequential(\n","  nn.Linear(in_features=4, out_features=256, bias=True)\n","  , nn.ReLU()\n","  , nn.Linear(in_features=256, out_features=128, bias=True)\n","  , nn.ReLU()\n","  , nn.Linear(in_features=128, out_features=n_actions, bias=True)\n",")\n","cuda = torch.device('cuda') \n","model.cuda()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=4, out_features=256, bias=True)\n","  (1): ReLU()\n","  (2): Linear(in_features=256, out_features=128, bias=True)\n","  (3): ReLU()\n","  (4): Linear(in_features=128, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"fvfC2FPU00PB","colab_type":"text"},"source":["#### Predict function"]},{"cell_type":"markdown","metadata":{"id":"JHmT7P5N00PD","colab_type":"text"},"source":["Note: output value of this function is not a torch tensor, it's a numpy array.\n","So, here gradient calculation is not needed.\n","<br>\n","Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n","to suppress gradient calculation.\n","<br>\n","Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n","<br>\n","With `.detach()` computational graph is built but then disconnected from a particular tensor,\n","so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n","<br>\n","In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."]},{"cell_type":"code","metadata":{"id":"bwHUIiYH00PF","colab_type":"code","colab":{}},"source":["from scipy.special import softmax\n","#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n","\n","@torch.no_grad()\n","def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    # convert states, compute logits, use softmax to get probability\n","    \"<YOUR CODE>\"\n","    n = states.shape[0]\n","    states = torch.tensor(states, dtype=torch.float32, device=cuda)\n","    qvalues = model(states) \n","    qvalues_proba = nn.functional.softmax(qvalues, dim=1)\n","\n","    \n","    return qvalues_proba.cpu().detach().numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wP4UjSrSHAkI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"3cf6466c-45d9-470b-83ae-bdeda9b6553a","executionInfo":{"status":"ok","timestamp":1590104752718,"user_tz":300,"elapsed":287,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"}}},"source":["print(env.reset())"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[ 0.04181674 -0.04082266  0.03336484  0.03606234]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0eAy35AA00PM","colab_type":"code","colab":{}},"source":["test_states = np.array([env.reset() for _ in range(5)])\n","test_probas = predict_probs(test_states)\n","assert isinstance(\n","    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n","assert tuple(test_probas.shape) == (\n","    test_states.shape[0], env.action_space.n), \"wrong output shape: {}\".format(np.shape(test_probas))\n","assert np.allclose(np.sum(test_probas, axis=1),\n","                   1), \"probabilities do not sum to 1\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SbCB1_DX00PT","colab_type":"text"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"VjhzWFjs00PV","colab_type":"code","colab":{}},"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    play a full session with REINFORCE agent and train at the session end.\n","    returns sequences of states, actions andrewards\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(np.array([s]))[0]\n","\n","        # Sample action with given probabilities.\n","        \"<YOUR CODE>\"\n","        a = np.random.choice((0, 1), p=action_probs)\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GXIJO4XZ00PZ","colab_type":"code","colab":{}},"source":["# test it\n","states, actions, rewards = generate_session(env)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N8nl96pb00Ph","colab_type":"text"},"source":["### Computing cumulative rewards"]},{"cell_type":"code","metadata":{"id":"h4cTvReR00Pi","colab_type":"code","colab":{}},"source":["def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    take a list of immediate rewards r(s,a) for the whole session \n","    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    The simple way to compute cumulative rewards is to iterate from last to first time tick\n","    and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    # <YOUR CODE>\n","    l = len(rewards)\n","    G = list(range(l))\n","\n","    G[l-1] = rewards[-1]\n","\n","    for t in reversed(range(l-1)):\n","      G[t] = rewards[t] + gamma * G[t+1]\n","   \n","  \n","    return G"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2WeqXo5g00Po","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"1866964b-20b5-4391-fe3d-38f5abe94e0b","executionInfo":{"status":"ok","timestamp":1590104758859,"user_tz":300,"elapsed":392,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"}}},"source":["get_cumulative_rewards(rewards)\n","assert len(get_cumulative_rewards(list(range(100)))) == 100\n","assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n","                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(get_cumulative_rewards(\n","    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(get_cumulative_rewards(\n","    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CfI4rl0O00Pu","colab_type":"text"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","\n","Following the REINFORCE algorithm, we can define our objective as follows: \n","\n","$$ \\hat J \\approx { 1 \\over N } \\sum_{s_i,a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G(s_i,a_i) $$\n","\n","When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient."]},{"cell_type":"code","metadata":{"id":"Qegrkl_w00Pw","colab_type":"code","colab":{}},"source":["def to_one_hot(y_tensor, ndims):\n","    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n","    y_tensor = y_tensor.type(torch.cuda.\n","                             LongTensor).view(-1, 1)\n","    y_one_hot = torch.zeros(\n","        y_tensor.size()[0], ndims, device=cuda).scatter_(1, y_tensor, 1)\n","    return y_one_hot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQrz4_hn00P3","colab_type":"code","colab":{}},"source":["# Your code: define optimizers\n","optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n","\n","\n","def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n","    \"\"\"\n","    Takes a sequence of states, actions and rewards produced by generate_session.\n","    Updates agent's weights by following the policy gradient above.\n","    Please use Adam optimizer with default parameters.\n","    Confer: https://fosterelli.co/entropy-loss-for-reinforcement-learning\n","    \"\"\"\n","\n","    # cast everything into torch tensors\n","    states = torch.tensor(states, dtype=torch.float32, device=cuda)\n","    actions = torch.tensor(actions, dtype=torch.int32, device=cuda)\n","    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n","    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32, device=cuda)\n","\n","    # predict logits, probas and log-probas using an agent.\n","    logits = model(states)\n","    probs = nn.functional.softmax(logits, -1)\n","    log_probs = nn.functional.log_softmax(logits, -1)\n","\n","    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n","        \"please use compute using torch tensors and don't use predict_probs function\"\n","\n","    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n","    log_probs_for_actions = torch.sum(\n","        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n","\n","   \n","    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n","    \"<YOUR CODE>\"\n","    entropy = -torch.sum(log_probs * probs).to(cuda)\n","    #print(entropy)\n","    \"<YOUR CODE>\"\n","    J_hat = torch.mean(log_probs_for_actions * cumulative_returns).to(cuda)\n","    # maximize J value, minimize entropy\n","    loss = - J_hat + entropy_coef * entropy   # this implementation does not use baseline\n","\n","\n","    # Gradient descent step\n","    \"<YOUR CODE>\"\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # technical: return session rewards to print them later\n","    return np.sum(rewards)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWJ-_4D200P9","colab_type":"text"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"id":"8El-n2iI00P-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":158},"outputId":"93106c62-0de6-4674-b788-5ca71c8dda2b","executionInfo":{"status":"ok","timestamp":1590104800932,"user_tz":300,"elapsed":38024,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"}}},"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env))\n","               for _ in range(100)]  # generate new sessions\n","    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n","    if np.mean(rewards) > 300:\n","        print(\"You Win!\")  # but you can train even further\n","        break"],"execution_count":14,"outputs":[{"output_type":"stream","text":["mean reward:34.170\n","mean reward:66.210\n","mean reward:145.330\n","mean reward:193.350\n","mean reward:32.390\n","mean reward:65.220\n","mean reward:374.330\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vtofyY_K00QF","colab_type":"text"},"source":["### Video"]},{"cell_type":"code","metadata":{"id":"Dk3lKedm00QG","colab_type":"code","colab":{}},"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cbt1WS0o00QM","colab_type":"code","colab":{"resources":{"http://localhost:8080/videos/openaigym.video.0.2236.video000064.mp4":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""}},"base_uri":"https://localhost:8080/","height":501},"outputId":"9fd355ea-e6fd-41b8-eb5f-52ea3ce88e32","executionInfo":{"status":"ok","timestamp":1590104851830,"user_tz":300,"elapsed":18683,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"}}},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"videos/openaigym.video.0.2236.video000064.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"osMNhZ1Z00QQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"9e9e6891-b20b-47bf-adb3-a33a35c8c300","executionInfo":{"status":"ok","timestamp":1590104867811,"user_tz":300,"elapsed":9073,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"}}},"source":["from submit import submit_cartpole_pytorch\n","submit_cartpole_pytorch(generate_session, 'agimenezzapiola@gmail.com', 'tZveBlKsoIrQin6Z')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Submitted to Coursera platform. See results on assignment page!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ne7v84hm00QW","colab_type":"text"},"source":["That's all, thank you for your attention!\n","\n","Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."]}]}